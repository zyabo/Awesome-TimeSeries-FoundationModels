# Awesome-TimeSeries-FoundationModels
Awesome resources and papers about leveraging Foundation Models for Time Series Analysis.

|      | Year | Model Name              | Paper Title                                                  | Paper URL                                 | Code/API/HuggingFace                                         | Authors                                                      |
| ---- | ---- | ----------------------- | ------------------------------------------------------------ | ----------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 1    | 2023 | TimeGPT-1               | TimeGPT-1                                                    | [Paper](https://arxiv.org/abs/2310.03589) | [API](https://github.com/Nixtla/nixtla)                      | [Nixtla](https://www.nixtla.io/)                             |
| 2    | 2023 | Lag-Llama               | Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting | [Paper](https://arxiv.org/abs/2310.08278) | [Code](https://github.com/time-series-foundation-models/lag-llama), [Hugging Face](https://huggingface.co/time-series-foundation-models/Lag-Llama) | [Kashif Rasul](https://arxiv.org/search/cs?searchtype=author&query=Rasul,+K),et al. |
| 3    | 2023 | TimesFM                 | A decoder-only foundation model for time-series forecasting  | [Paper](https://arxiv.org/abs/2310.10688) | [Code](https://github.com/google-research/timesfm), [Hugging Face](https://huggingface.co/google/timesfm-1.0-200m) | Google（Abhimanyu Das , et al.）                             |
| 4    | 2024 | Tiny Time Mixers (TTMs) | Tiny Time Mixers (TTMs): Fast Pre-trained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series | [Paper](https://arxiv.org/abs/2401.03955) | [Hugging Face](https://huggingface.co/ibm-granite/granite-timeseries-ttm-v1) | [Vijay Ekambaram](https://arxiv.org/search/cs?searchtype=author&query=Ekambaram,+V),et al. |
| 5    | 2024 | Moirai                  | Unified Training of Universal Time Series Forecasting Transformers | [Paper](https://arxiv.org/abs/2402.02592) | [Code](https://github.com/SalesforceAIResearch/uni2ts), [Hugging Face](https://huggingface.co/Salesforce/moirai-1.0-R-large) | [Gerald Woo](https://arxiv.org/search/cs?searchtype=author&query=Woo,+G), et al. |
| 6    | 2024 | Chronos                 | Chronos: Learning the Language of Time Series by Amazon      | [Paper](https://arxiv.org/abs/2403.07815) | [Code](https://github.com/amazon-science/chronos-forecasting), [Hugging Face](https://huggingface.co/collections/amazon/chronos-models-65f1791d630a8d57cb718444) | [Abdul Fatir Ansari](https://arxiv.org/search/cs?searchtype=author&query=Ansari,+A+F),et al. |
| 7    | 2024 | Timer                   | Timer: Generative Pre-trained Transformers Are Large Time Series Models | [Paper](https://arxiv.org/abs/2402.02368) | [Code](https://github.com/thuml/Large-Time-Series-Model?tab=readme-ov-file) | [Yong Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu,+Y), et al. |
| 8    | 2024 | TOTEM                   | TOTEM: TOkenized Time Series EMbeddings for General Time Series Analysis | [Paper](https://arxiv.org/abs/2402.16412) | [Code](https://github.com/SaberaTalukder/TOTEM)              | [Sabera Talukder](https://arxiv.org/search/cs?searchtype=author&query=Talukder,+S),  et al. |

